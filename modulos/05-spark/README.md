# üî• M√≥dulo 5: Apache Spark - Processamento Distribu√≠do

## üîç Sobre este M√≥dulo
Este m√≥dulo aborda os conceitos e pr√°ticas do Apache Spark para processamento distribu√≠do de dados, focando no desenvolvimento de aplica√ß√µes com PySpark e na implementa√ß√£o de processamento eficiente em larga escala.

## üìã √çndice

- [Objetivos](#-objetivos-do-m√≥dulo)
- [Fundamentos do Spark](#1-fundamentos-do-spark)
- [PySpark DataFrame API](#2-pyspark-dataframe-api)
- [Otimiza√ß√£o de Performance](#3-otimiza√ß√£o-de-performance)
- [Integra√ß√£o com Modern Data Stack](#4-integra√ß√£o-com-modern-data-stack)
- [Exerc√≠cios Pr√°ticos](#-exerc√≠cios-pr√°ticos)
- [Recursos Adicionais](#-recursos-adicionais)
- [Quiz](#-quiz)
- [Projeto do M√≥dulo](#-projeto-do-m√≥dulo)

## üéØ Objetivos do M√≥dulo
- Compreender os fundamentos do Apache Spark
- Desenvolver aplica√ß√µes Spark em Python (PySpark)
- Implementar processamento distribu√≠do eficiente
- Integrar Spark com outras ferramentas da stack

## üìã Conte√∫do

### 1. Fundamentos do Spark

%%{init: { "themeVariables": { "fontFamily": "Arial", "fontSize": "10px" } }}%%
```mermaid
graph TD
    A[Driver] --> B[SparkContext]
    B --> C[Executors]
    C --> D[Tasks]
    
    E[RDD] --> F[DataFrame]
    F --> G[SQL]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#bfb,stroke:#333,stroke-width:2px
    style D fill:#fbf,stroke:#333,stroke-width:2px
```

#### 1.1 Conceitos B√°sicos
- RDD (Resilient Distributed Dataset)
- DataFrame e Dataset
- Transforma√ß√µes e A√ß√µes
- Particionamento
- Shuffle
- Persist√™ncia

#### 1.2 Arquitetura Spark

%%{init: { "themeVariables": { "fontFamily": "Arial", "fontSize": "10px" } }}%%
```mermaid
flowchart LR
    A[Driver Program] --> B[Cluster Manager]
    B --> C[Worker Node 1]
    B --> D[Worker Node 2]
    B --> E[Worker Node N]
    
    style A fill:#f9f,stroke:#333,stroke-width:2px
    style B fill:#bbf,stroke:#333,stroke-width:2px
    style C fill:#bfb,stroke:#333,stroke-width:2px
    style D fill:#fbf,stroke:#333,stroke-width:2px
    style E fill:#ff9,stroke:#333,stroke-width:2px
```

### 2. PySpark DataFrame API

#### 2.1 Opera√ß√µes B√°sicas
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *

# Criar SparkSession
spark = SparkSession.builder \
    .appName("ExemploPySpark") \
    .getOrCreate()

# Ler dados
df = spark.read.csv("dados.csv", header=True)

# Transforma√ß√µes
df_transformed = df \
    .select("coluna1", "coluna2") \
    .filter(col("coluna1") > 0) \
    .groupBy("coluna2") \
    .agg(sum("coluna1").alias("total"))

# A√ß√µes
df_transformed.show()
```

#### 2.2 Window Functions
```python
from pyspark.sql.window import Window

# Definir janela
window_spec = Window \
    .partitionBy("categoria") \
    .orderBy("valor") \
    .rowsBetween(Window.unboundedPreceding, Window.currentRow)

# Aplicar fun√ß√£o de janela
df_window = df \
    .withColumn("media_movel", 
                avg("valor").over(window_spec))
```

### 3. Otimiza√ß√£o de Performance

#### 3.1 Estrat√©gias de Otimiza√ß√£o
- Particionamento
- Caching
- Broadcast Joins
- Predicate Pushdown
- Project Pushdown

#### 3.2 Exemplo de Otimiza√ß√£o
```python
# Broadcast Join
small_df = spark.table("small_table")
large_df = spark.table("large_table")

# Broadcast a tabela menor
from pyspark.sql.functions import broadcast
optimized_df = large_df.join(broadcast(small_df), "key")

# Persist√™ncia
optimized_df.persist()
```

### 4. Integra√ß√£o com Modern Data Stack

#### 4.1 Spark com Airflow
```python
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator

spark_job = SparkSubmitOperator(
    task_id='processar_dados',
    application='job.py',
    conf={
        'spark.driver.memory': '4g',
        'spark.executor.memory': '4g',
        'spark.executor.cores': '2'
    }
)
```

#### 4.2 Spark com Delta Lake
```python
# Configurar Delta Lake
spark = SparkSession.builder \
    .config("spark.jars.packages", "io.delta:delta-core_2.12:1.0.0") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .getOrCreate()

# Ler/Escrever Delta
df.write.format("delta").save("/path/to/delta/table")
df = spark.read.format("delta").load("/path/to/delta/table")
```

## üíª Exerc√≠cios Pr√°ticos

### Exerc√≠cio 1: Processamento B√°sico
1. Criar um job Spark que:
   - L√™ dados de m√∫ltiplas fontes
   - Aplica transforma√ß√µes
   - Realiza agrega√ß√µes
   - Salva resultados

### Exerc√≠cio 2: An√°lise Avan√ßada
1. Desenvolver an√°lises com:
   - Window Functions
   - UDFs
   - Joins complexos
   - Otimiza√ß√µes

### Exerc√≠cio 3: Integra√ß√£o
1. Implementar pipeline que:
   - Integra com Airflow
   - Usa Delta Lake
   - Conecta com Data Warehouse
   - Monitora performance

## üìö Recursos Adicionais

### Documenta√ß√£o
- [Apache Spark Documentation](https://spark.apache.org/docs/latest/)
- [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)
- [Delta Lake Documentation](https://docs.delta.io/latest/index.html)

### Artigos
- [Spark Performance Tuning](https://spark.apache.org/docs/latest/tuning.html)
- [Best Practices](https://spark.apache.org/docs/latest/sql-performance-tuning.html)

### V√≠deos
- [Spark Fundamentals](https://www.youtube.com/watch?example1)
- [Advanced Spark Patterns](https://www.youtube.com/watch?example2)

## ‚úÖ Quiz

1. Qual a diferen√ßa entre RDD e DataFrame no Spark?
2. Como funciona o lazy evaluation no Spark?
3. Quais s√£o as estrat√©gias de particionamento no Spark?
4. Como otimizar joins no Spark?
5. Quando usar cache vs persist no Spark?

## üéØ Projeto do M√≥dulo

### An√°lise de Dados de Voos em Larga Escala

Desenvolva uma aplica√ß√£o Spark que:

1. Processamento
   - Ingest√£o de dados brutos
   - Limpeza e valida√ß√£o
   - Transforma√ß√µes complexas
   - Agrega√ß√µes e an√°lises

2. Otimiza√ß√£o
   - Particionamento adequado
   - Estrat√©gias de cache
   - Otimiza√ß√£o de joins
   - Monitoramento de performance

3. Integra√ß√µes
   - Delta Lake para versionamento
   - Airflow para orquestra√ß√£o
   - Data Warehouse para armazenamento
   - Metabase para visualiza√ß√£o

Requisitos:
- C√≥digo eficiente e otimizado
- Documenta√ß√£o detalhada
- Testes de performance
- Monitoramento completo
- Integra√ß√£o com stack

## üìù Avalia√ß√£o
- Exerc√≠cios pr√°ticos: 30%
- Quiz: 20%
- Projeto do m√≥dulo: 50%

## üîÑ Pr√≥ximos Passos
No pr√≥ximo m√≥dulo, aprenderemos sobre BigQuery e Data Warehouse, focando em armazenamento e an√°lise de dados em larga escala. 